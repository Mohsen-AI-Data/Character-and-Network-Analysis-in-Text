{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ad5738",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a77bd5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140659d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e685311b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0931cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mohsenrahimi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests    # download the book \n",
    "import re          #`re` (regular expressions) library to remove unwanted characters and information from the text\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "import contractions\n",
    "from itertools import combinations\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import networkx as nx\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97bc0b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book downloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests    # download the book \n",
    "book_id = 1342\n",
    "url = f\"https://www.gutenberg.org/files/{book_id}/{book_id}-0.txt\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    with open(\"book.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(response.text)\n",
    "    print(\"Book downloaded successfully.\")\n",
    "else:\n",
    "    print(\"Failed to download the book.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "034b74f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"book.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    book_content = f.read()                        #`book_content` now contains the entire text of the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60c1c5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The header is :\n",
      " ï»¿The Project Gutenberg eBook of Pride and prejudice, by Jane Austen\n",
      "\n",
      "This eBook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\n",
      "of the Project Gutenberg License included with this eBook or online at\n",
      "www.gutenberg.org. If you are not located in the United States, you\n",
      "will have to check the laws of the country where you are located before\n",
      "usin\n",
      "\n",
      "The footer is :\n",
      " by copyright in\n",
      "the U.S. unless a copyright notice is included. Thus, we do not\n",
      "necessarily keep eBooks in compliance with any particular paper\n",
      "edition.\n",
      "\n",
      "Most people start at our website which has the main PG search\n",
      "facility: www.gutenberg.org\n",
      "\n",
      "This website includes information about Project Gutenberg-tm,\n",
      "including how to make donations to the Project Gutenberg Literary\n",
      "Archive Foundation, how to help produce our new eBooks, and how to\n",
      "subscribe to our email newsletter to hear about new eBooks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the header and footer of the book\n",
    "print(\"The header is :\\n\",book_content[:500])\n",
    "\n",
    "print(\"\\nThe footer is :\\n\",book_content[-500:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91df6c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove page numbers, headers, and footers:\n",
    "\n",
    "book_content = re.sub(r\"^(.*?)\\*{3} START OF (.*?)\\*{3}\", \"\", book_content, flags=re.DOTALL)  # Remove header\n",
    "book_content = re.sub(r\"\\*{3} END OF (.*?)\\*{3}(\\s|.)*$\", \"\", book_content, flags=re.DOTALL)  # Remove footer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "794ded61",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The header is :\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                            [Illustration:\n",
      "\n",
      "                             GEORGE ALLEN\n",
      "                               PUBLISHER\n",
      "\n",
      "                        156 CHARING CROSS ROAD\n",
      "                                LONDON\n",
      "\n",
      "                             RUSKIN HOUSE\n",
      "                                   ]\n",
      "\n",
      "\n",
      "The footer is :\n",
      " e means of uniting them.\n",
      "\n",
      "                            [Illustration:\n",
      "\n",
      "                                  THE\n",
      "                                  END\n",
      "                                   ]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "             CHISWICK PRESS:--CHARLES WHITTINGHAM AND CO.\n",
      "                  TOOKS COURT, CHANCERY LANE, LONDON.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the content of the book after removing header and footer\n",
    "\n",
    "print(\"The header is :\\n\",book_content[:300])\n",
    "print(\"\\nThe footer is :\\n\",book_content[-300:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c4ef98",
   "metadata": {},
   "source": [
    "It seems that the regular expressions used to remove the header and footer didn't work as expected. Let's try an alternative approach using the actual content of the book.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd6dcbf9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              78\n",
      "\n",
      "Heading to Chapter XIV.                                               84\n",
      "\n",
      "âProtested that he never read novelsâ                                 87\n",
      "\n",
      "Heading to Chapter XV.                                                89\n",
      "\n",
      "Heading to Chapter XVI.                                               95\n",
      "\n",
      "âThe officers of the ----shireâ                                       97\n",
      "\n",
      "âDelighted to see their dear friend againâ                           108\n",
      "\n",
      "Heading to Chapter XVIII.                                            113\n",
      "\n",
      "âSuch very superior dancing is not often seenâ                       118\n",
      "\n",
      "âTo assure you in the most animated languageâ                        132\n",
      "\n",
      "Heading to Chapter XX.                                               139\n",
      "\n",
      "âThey entered the breakfast-roomâ                                    143\n",
      "\n",
      "Heading to Chapter XXI.                                              146\n",
      "\n",
      "âWalked back with themâ                                              148\n",
      "\n",
      "Heading to Chapter XXII.                                             154\n",
      "\n",
      "âSo much love and eloquenceâ                                         156\n",
      "\n",
      "âProtested he must be entirely mistakenâ                             161\n",
      "\n",
      "âWhenever she spoke in a low voiceâ                                  166\n",
      "\n",
      "Heading to Chapter XXIV.                                             168\n",
      "\n",
      "Heading to Chapter XXV.                                              175\n",
      "\n",
      "âOffended two or three young ladiesâ                                 177\n",
      "\n",
      "âWill you come and see me?â                                          181\n",
      "\n",
      "âOn the stairsâ                                                      189\n",
      "\n",
      "âAt the doorâ                                                        194\n",
      "\n",
      "âIn conversation with the ladiesâ                                    198\n",
      "\n",
      "âLady Catherine,â said she, âyou have given me a treasureâ           200\n",
      "\n",
      "Heading to Chapter XXX.                                              209\n",
      "\n",
      "âHe never failed to inform themâ                                     211\n",
      "\n",
      "âThe gentlemen accompanied himâ                                      213\n",
      "\n",
      "Heading to Chapter XXXI.                                             215\n",
      "\n",
      "Heading to Chapter XXXII.                                            221\n",
      "\n",
      "âAccompanied by their auntâ                                          225\n",
      "\n",
      "âOn looking upâ                                                      228\n",
      "\n",
      "Heading to Chapter XXXIV.                                            235\n",
      "\n",
      "âHearing herself calledâ                                             243\n",
      "\n",
      "Heading to Chapter XXXVI.                                            253\n",
      "\n",
      "âMeeting accidentally in townâ                                       256\n",
      "\n",
      "âHis parting obeisanceâ                                              261\n",
      "\n",
      "âDawsonâ                                                             263\n",
      "\n",
      "âThe elevation of his feelingsâ                                      267\n",
      "\n",
      "âThey had forgotten to leave any messageâ                            270\n",
      "\n",
      "âHow nicely we are crammed in!â                                      272\n",
      "\n",
      "Heading to Chapter XL.                                               278\n",
      "\n",
      "âI am determined never to speak of it againâ                         283\n",
      "\n",
      "âWhen Colonel Millerâs regiment went awayâ                           285\n",
      "\n",
      "âTenderly flirtingâ                                                  290\n",
      "\n",
      "The arrival of the Gardiners                                         294\n",
      "\n",
      "âConjecturing as to the dateâ                                        301\n",
      "\n",
      "Heading to Chapter XLIV.                                             318\n",
      "\n",
      "âTo make herself agreeable to allâ                                   321\n",
      "\n",
      "âEngaged by the riverâ                                               327\n",
      "\n",
      "Heading to Chapter XLVI.                                             334\n",
      "\n",
      "âI have not an instant to loseâ                                      339\n",
      "\n",
      "âThe first pleasing earnest of their welcomeâ                        345\n",
      "\n",
      "The Post                                                             359\n",
      "\n",
      "âTo whom I have related the affairâ                                  363\n",
      "\n",
      "Heading to Chapter XLIX.                                             368\n",
      "\n",
      "âBut perhaps you would like to read itâ                              370\n",
      "\n",
      "âThe spiteful old ladiesâ                                            377\n",
      "\n",
      "âWith an affectionate smileâ                                         385\n",
      "\n",
      "âI am sure she did not listenâ                                       393\n",
      "\n",
      "âMr. Darcy with himâ                                                 404\n",
      "\n",
      "âJane happened to look roundâ                                        415\n",
      "\n",
      "âMrs. Long and her niecesâ                                           420\n",
      "\n",
      "âLizzy, my dear, I want to speak to youâ                             422\n",
      "\n",
      "Heading to Chapter LVI.                                              431\n",
      "\n",
      "âAfter a short surveyâ                                               434\n",
      "\n",
      "âBut now it comes outâ                                               442\n",
      "\n",
      "âThe efforts of his auntâ                                            448\n",
      "\n",
      "âUnable to utter a syllableâ                                         457\n",
      "\n",
      "âThe obsequious civilityâ                                            466\n",
      "\n",
      "Heading to Chapter LXI.                                              472\n",
      "\n",
      "The End                                                              476\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Illustration: Â·PRIDE AND PREJUDICEÂ·\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chapter I.]\n",
      "\n",
      "\n",
      "It is a truth universally acknowledged, that a single man in possession\n",
      "of a good fortune must be in want of a wife.\n",
      "\n",
      "However little known the feelings or views of such a man may be on his\n",
      "first entering a neighbourhood, this truth is so well fixed in the minds\n",
      "of the surrounding families, that he is considered as the rightful\n",
      "property of some one or other of their daughters.\n",
      "\n",
      "âMy dear Mr. Bennet,â said his lady to him one day, âhave you heard that\n",
      "Netherfield Park is let at last?â\n",
      "\n",
      "Mr. Bennet replied that he had not.\n",
      "\n",
      "âBut it is,â returned she; âfor Mrs. Long has just been here, and she\n",
      "told me all about it.â\n",
      "\n",
      "Mr. Bennet made no answer.\n",
      "\n",
      "âDo not you want to know who has taken it?â cried his wife, impatiently.\n",
      "\n",
      "â_You_ want to tell me, and I have no objection to hearing it.â\n",
      "\n",
      "[Illustration:\n",
      "\n",
      "âHe came down to see the placeâ\n",
      "\n",
      "[_Copyright 1894 by George Allen._]]\n",
      "\n",
      "This was invitation enough.\n",
      "\n",
      "âWhy, my dear, you must know, Mrs. Long says that Netherfield is taken\n",
      "by a young man of large fortune from the north of England; that he came\n",
      "down on Monday in a chaise and four to see the place, and was so much\n",
      "delighted with it that he agreed with Mr. Morris immediately; that he is\n",
      "to take possession before Michaelmas, and some of his servants are to be\n",
      "in the house by the end of next week.â\n",
      "\n",
      "âWhat is his name?â\n",
      "\n",
      "âBingley.â\n",
      "\n",
      "âIs he married or single?â\n",
      "\n",
      "âOh, single, my dear, to be sure! A single man of large fortune; four or\n",
      "five thousand a year. What a fine thing for our girls!â\n",
      "\n",
      "âHow so? how can it affect them?â\n",
      "\n",
      "âMy dear Mr. Bennet,â replied his wife, âhow can you be so tiresome? You\n",
      "must know that I am thinking of his marrying one of them.â\n",
      "\n",
      "âIs that his design in settling here?â\n",
      "\n",
      "âDesign? Nonsense, how can you talk so! But it is very likely that he\n",
      "_may_ fall in love with one of them, and therefore you must visit him as\n",
      "soon as he comes.â\n",
      "\n",
      "âI see no occasion for that. You and the girls may go--or you may send\n",
      "them by themselves, which perhaps will be still better; for as you are\n",
      "as handsome as any of them, Mr. Bingley might like you the best of the\n",
      "party.â\n",
      "\n",
      "âMy dear, you flatter me. I certainly _have_ had my share of beauty, but\n",
      "I do not pretend to be anything extraordinary now. When a woman has five\n",
      "grown-up daughters, she ought to give over thinking of her own beauty.â\n",
      "\n",
      "âIn such cases, a woman has not often much beauty to think of.â\n",
      "\n",
      "âBut, my dear, you must indeed go and see Mr. Bingley when he comes into\n",
      "the neighbourhood.â\n",
      "\n",
      "âIt is more than I engage for, I assure you.â\n",
      "\n",
      "âBut consider your daughters. Only think what an establishment it would\n",
      "be for one of them. Sir William and Lady Lucas are determined to go,\n",
      "merely on that account; for in general, you know, they visit no new\n",
      "comers. Indeed you must go, for it will be impossible for _us_ to visit\n",
      "him, if you do not.â\n",
      "\n",
      "âYou are over scrupulous, surely. I dare say Mr. Bingley will be very\n",
      "glad to see you; and I will send a few lines by you to assure him of my\n",
      "hearty consent to his marrying whichever he chooses of the girls--though\n",
      "I must throw in a good word for my little Lizzy.â\n",
      "\n",
      "âI desire you will do no such thing. Lizzy is not a bit better than the\n",
      "others: and I am sure she is not half so handsome as Jane, nor half so\n",
      "good-humoured as Lydia. But you are always giving _her_ the preference.â\n",
      "\n",
      "âThey have none of them much to recommend them,â replied he: âthey are\n",
      "all silly and ignorant like other girls; but Lizzy has something more of\n",
      "quickness than her sisters.â\n",
      "\n",
      "âMr. Bennet, how can you abuse your own children in such a way? You take\n",
      "delight in vexing me. You have no compassion on my poor nerves.â\n",
      "\n",
      "âYou mistake me, my dear. I have a high respect for your nerves. They\n",
      "are my old friends. I have heard you mention them with consideration\n",
      "these twenty years at least.â\n",
      "\n",
      "âAh, you do not know what I suffer.â\n",
      "\n",
      "âBut I hope you will get over it, and live to see many young men of four\n",
      "thousand a year come into the neighbourhood.â\n",
      "\n",
      "âIt will be no use to us, if twenty such should come, since you will not\n",
      "visit them.â\n",
      "\n",
      "âDepend upon it, my dear, that when there are twenty, I will visit them\n",
      "all.â\n",
      "\n",
      "Mr. Bennet was so odd a mixture of quick parts, sarcastic humour,\n",
      "reserve, and caprice, that the experience of three-and-twen\n"
     ]
    }
   ],
   "source": [
    "# Identify the start phrase\n",
    "with open(\"book.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    book_content = f.read()                        #`book_content` now contains the entire text of the book\n",
    " \n",
    "# In below in each time I printed 10,000 characters to find out the starting point\n",
    "\n",
    "#print(book_content[1:30000])\n",
    "print(book_content[30000:40000])\n",
    "#print(book_content[40000:50000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3fb1b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The starting phrase is `Chapter I.]`\n"
     ]
    }
   ],
   "source": [
    "print(\"The starting phrase is `Chapter I.]`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc05ac07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " had never before fallen in\n",
      "her way. By Elizabethâs instructions she began to comprehend that a\n",
      "woman may take liberties with her husband, which a brother will not\n",
      "always allow in a sister more than ten years younger than himself.\n",
      "\n",
      "Lady Catherine was extremely indignant on the marriage of her nephew;\n",
      "and as she gave way to all the genuine frankness of her character, in\n",
      "her reply to the letter which announced its arrangement, she sent him\n",
      "language so very abusive, especially of Elizabeth, that for some time\n",
      "all intercourse was at an end. But at length, by Elizabethâs persuasion,\n",
      "he was prevailed on to overlook the offence, and seek a reconciliation;\n",
      "and, after a little further resistance on the part of his aunt, her\n",
      "resentment gave way, either to her affection for him, or her curiosity\n",
      "to see how his wife conducted herself; and she condescended to wait on\n",
      "them at Pemberley, in spite of that pollution which its woods had\n",
      "received, not merely from the presence of such a mistress, but the\n",
      "visits of her uncle and aunt from the city.\n",
      "\n",
      "With the Gardiners they were always on the most intimate terms. Darcy,\n",
      "as well as Elizabeth, really loved them; and they were both ever\n",
      "sensible of the warmest gratitude towards the persons who, by bringing\n",
      "her into Derbyshire, had been the means of uniting them.\n",
      "\n",
      "                            [Illustration:\n",
      "\n",
      "                                  THE\n",
      "                                  END\n",
      "                                   ]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "             CHISWICK PRESS:--CHARLES WHITTINGHAM AND CO.\n",
      "                  TOOKS COURT, CHANCERY LANE, LONDON.\n",
      "\n",
      "*** END OF THE PROJECT GUTENBERG EBOOK PRIDE AND PREJUDICE ***\n",
      "\n",
      "Updated editions will replace the previous one--the old editions will\n",
      "be renamed.\n",
      "\n",
      "Creating the works from print editions not protected by U.S. copyright\n",
      "law means that no one owns a United States copyright in these works,\n",
      "so the Foundation (and you!) can copy and distribute it in the\n",
      "United States without permission and without paying copyright\n",
      "royalties. Special rules, set forth in the General Terms of Use part\n",
      "of this license, apply to copying and distributing Project\n",
      "Gutenberg-tm electronic works to protect the PROJECT GUTENBERG-tm\n",
      "concept and trademark. Project Gutenberg is a registered trademark,\n",
      "and may not be used if you charge for an eBook, except by following\n",
      "the terms of the trademark license, including paying royalties for use\n",
      "of the Project Gutenberg trademark. If you do not charge anything for\n",
      "copies of this eBook, complying with the trademark license is very\n",
      "easy. You may use this eBook for nearly any purpose such as creation\n",
      "of derivative works, reports, performances and research. Project\n",
      "Gutenberg eBooks may be modified and printed and given away--you may\n",
      "do practically ANYTHING in the United States with eBooks not protected\n",
      "by U.S. copyright law. Redistribution is subject to the trademark\n",
      "license, especially commercial redistribution.\n",
      "\n",
      "START: FULL LICENSE\n",
      "\n",
      "THE FULL PROJECT GUTENBERG LICENSE\n",
      "PLEASE READ THIS BEFORE YOU DISTRIBUTE OR USE THIS WORK\n",
      "\n",
      "To protect the Project Gutenberg-tm mission of promoting the free\n",
      "distribution of electronic works, by using or distributing this work\n",
      "(or any other work associated in any way with the phrase \"Project\n",
      "Gutenberg\"), you agree to comply with all the terms of the Full\n",
      "Project Gutenberg-tm License available with this file or online at\n",
      "www.gutenberg.org/license.\n",
      "\n",
      "Section 1. General Terms of Use and Redistributing Project\n",
      "Gutenberg-tm electronic works\n",
      "\n",
      "1.A. By reading or using any part of this Project Gutenberg-tm\n",
      "electronic work, you indicate that you have read, understand, agree to\n",
      "and accept all the terms of this license and intellectual property\n",
      "(trademark/copyright) agreement. If you do not agree to abide by all\n",
      "the terms of this agreement, you must cease using and return or\n",
      "destroy all copies of Project Gutenberg-tm electronic works in your\n",
      "possession. If you paid a fee for obtaining a copy of or access to a\n",
      "Project Gutenberg-tm electronic work and you do not agree to be bound\n",
      "by the terms of this agreement, you may obtain a refund from the\n",
      "person or entity to whom you paid the fee as set forth in paragraph\n",
      "1.E.8.\n",
      "\n",
      "1.B. \"Project Gutenberg\" is a registered trademark. It may only be\n",
      "used on or associated in any way with an electronic work by people who\n",
      "agree to be bound by the terms of this agreement. There are a few\n",
      "things that you can do with most Project Gutenberg-tm electronic works\n",
      "even without complying with the full terms of this agreement. See\n",
      "paragraph 1.C below. There are a lot of things you can do with Project\n",
      "Gutenberg-tm electronic works if you follow the terms of this\n",
      "agreement and help preserve free future access to Project Gutenberg-tm\n",
      "electronic works. See paragraph 1.E below.\n",
      "\n",
      "1.C. The Project Gutenberg Literary Archive Foundation (\"the\n",
      "Foundation\" or PGLAF), owns a compilation copyright in the collection\n",
      "of Project Gutenberg-tm electronic works. Nearly all the individual\n",
      "works in the collection are in the public\n"
     ]
    }
   ],
   "source": [
    "# Identify the end phrase of the actual content\n",
    "#print(book_content[-5000:])\n",
    "#print(book_content[-10000:-5000])\n",
    "#print(book_content[-15000:-10000])\n",
    "print(book_content[-20000:-15000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6596d5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ending phrase is `THE END`\n"
     ]
    }
   ],
   "source": [
    "print(\"The ending phrase is `THE END`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f679eb",
   "metadata": {},
   "source": [
    "#### Identify the start and end phrases of the actual content:\n",
    "\n",
    "For this specific book (Pride and Prejudice), the actual content starts after the phrase `CHAPTER II` and \n",
    "ends before the phrase `THE END`. I can use these phrases to extract the main content: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b6de26d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check the content of the book after removing header and footer\n",
      "\n",
      "The content is stated from:\n",
      "\n",
      " Chapter I.]\n",
      "\n",
      "\n",
      "It is a truth universally acknowledged, that a single man in possession\n",
      "of a good fortune must be in want of a wife.\n",
      "\n",
      "However little known the feelings or views of such a man may be on his\n",
      "first entering a neighbourhood, this truth is so well fixed in the minds\n",
      "of the surrounding families, that he is considered as the rightful\n",
      "property of some one or other of their daughters.\n",
      "\n",
      "âMy dear Mr. Bennet,â said his lady to him one day, âhave you heard that\n",
      "Netherfield Park is let at \n",
      "\n",
      "The content is ended at:\n",
      "\n",
      " most intimate terms. Darcy,\n",
      "as well as Elizabeth, really loved them; and they were both ever\n",
      "sensible of the warmest gratitude towards the persons who, by bringing\n",
      "her into Derbyshire, had been the means of uniting them.\n",
      "\n",
      "                            [Illustration:\n",
      "\n",
      "                                  THE\n",
      "                                  END\n",
      "                                   ]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "             CHISWICK PRESS:--CHARLES WHITTINGHAM AND CO.\n",
      "                  TOOKS COURT, CHANCERY LANE, LONDON.\n",
      "\n",
      "*** \n"
     ]
    }
   ],
   "source": [
    "with open(\"book.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    book_content = f.read()                        #`book_content` now contains the entire text of the book\n",
    " \n",
    "\n",
    "start_phrase = \"Chapter I.]\"\n",
    "end_phrase = \"END OF THE PROJECT GUTENBERG EBOOK PRIDE AND PREJUDICE \"\n",
    "\n",
    "start_index = book_content.find(start_phrase)\n",
    "end_index = book_content.find(end_phrase)\n",
    "\n",
    "book_content = book_content[start_index:end_index]\n",
    "\n",
    "print(\"Check the content of the book after removing header and footer\\n\")\n",
    "print(\"The content is stated from:\\n\\n\",book_content[:500])\n",
    "print(\"\\nThe content is ended at:\\n\\n\",book_content[-500:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a6bfbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chapters: 60\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Add an extra \"CHAPTER\" keyword at the end of the book for the last chapter\n",
    "#adding the \"CHAPTER\" keyword at the end. It seems like you have added it to ensure that the last chapter is \n",
    "#included correctly when splitting.\n",
    "\n",
    "book_content_with_last_chapter = book_content + \"CHAPTER\"\n",
    "\n",
    "# Split the book into chapters using the \"CHAPTER\" keyword\n",
    "chapters = book_content_with_last_chapter.split(\"CHAPTER\")\n",
    "\n",
    "# Remove the first element, which is just the text before the first chapter\n",
    "#chapters.pop(0)\n",
    "\n",
    "# Remove the last element, which is just the text after the last chapter\n",
    "chapters.pop()\n",
    "\n",
    "# Print the number of chapters and a sample chapter\n",
    "print(f\"Total chapters: {len(chapters)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "105573c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chapter 1 starts from:\n",
      " Chapter I.]\n",
      "\n",
      "\n",
      "It is a truth universally acknowledged, that a single man in possession\n",
      "of a good fortune must be in want of a wife.\n",
      "\n",
      "However little known the feelings or views of such a man may be on his\n",
      "first entering a neighbourhood, this truth is so well fixed in the minds\n",
      "of the surrounding famil\n",
      "\n",
      "Chapter 59 ends at:\n",
      "\n",
      " most intimate terms. Darcy,\n",
      "as well as Elizabeth, really loved them; and they were both ever\n",
      "sensible of the warmest gratitude towards the persons who, by bringing\n",
      "her into Derbyshire, had been the means of uniting them.\n",
      "\n",
      "                            [Illustration:\n",
      "\n",
      "                                  THE\n",
      "                                  END\n",
      "                                   ]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "             CHISWICK PRESS:--CHARLES WHITTINGHAM AND CO.\n",
      "                  TOOKS COURT, CHANCERY LANE, LONDON.\n",
      "\n",
      "*** \n"
     ]
    }
   ],
   "source": [
    "# print the start and end of first and last chapter respectively\n",
    "\n",
    "print(\"\\nChapter 1 starts from:\\n\", chapters[0][:300])\n",
    "print(\"\\nChapter 59 ends at:\\n\\n\", chapters[59][-500:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7bd4068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Calculate the number of words in each chapter\n",
    "word_counts = [len(chapter.split()) for chapter in chapters]\n",
    "\n",
    "# Create a DataFrame with the chapter number and word count\n",
    "chapter_word_counts = pd.DataFrame({\"Chapter\": range(1, len(chapters) + 1), \"Word Count\": word_counts})\n",
    "#print(chapter_word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08908e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#Remove special characters and numbers,    we use the `re.sub()` function. \n",
    "\n",
    "#This example removes all non-alphabetic characters except spaces and newline characters:\n",
    "\n",
    "#book_content_clean = re.sub(r\"[^a-zA-Z\\s\\n]\", \"\", book_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda3cf4b",
   "metadata": {},
   "source": [
    "\n",
    "This modified regular expression retains alphabetic characters, spaces, newline characters, \n",
    "and the following punctuation marks: \".\", \",\", \";\", \"!\", \"?\", \"'\", and \"\\\"\". This way, we'll preserve \n",
    "sentence boundaries, abbreviations, and other useful punctuation marks while still removing other special \n",
    "characters and numbers that might not be relevant to your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "472ed28a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Chapter 1 as below:\n",
      " Chapter I.\n",
      "\n",
      "\n",
      "It is a truth universally acknowledged, that a single man in possession\n",
      "of a good fortune must be in want of a wife.\n",
      "\n",
      "However little known the feelings or views of such a man may be on his\n",
      "first entering a neighbourhood, this truth is so well fixed in the minds\n",
      "of the surrounding famili\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# Function to clean the content of a chapter\n",
    "def clean_chapter_content(chapter_content):\n",
    "    return re.sub(r\"[^a-zA-Z\\s\\n.,;!?'\\\"]\", \"\", chapter_content)\n",
    "\n",
    "# Apply the cleaning function to all chapters\n",
    "chapters_clean = [clean_chapter_content(chapter) for chapter in chapters]\n",
    "\n",
    "# Print a sample cleaned chapter\n",
    "print(\"Cleaned Chapter 1 as below:\\n\", chapters_clean[0][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e455048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter 1 without contractions:\n",
      " Chapter I.\n",
      "\n",
      "\n",
      "It is a truth universally acknowledged, that a single man in possession\n",
      "of a good fortune must be in want of a wife.\n",
      "\n",
      "However little known the feelings or views of such a man may be on his\n",
      "first entering a neighbourhood, this truth is so well fixed in the minds\n",
      "of the surrounding families, that he is considered as the rightful\n",
      "property of some one or other of their daughters.\n",
      "\n",
      "My dear Mr. Bennet, said his lady to him one day, have you heard that\n",
      "Netherfield Park is let at last?\n",
      "\n",
      "Mr.\n"
     ]
    }
   ],
   "source": [
    "# apply the `contractions.fix()` function to each chapter (e.g., converting \"don't\" to \"do not\")\n",
    "\n",
    "import contractions\n",
    "\n",
    "chapters_no_contractions = [contractions.fix(chapter) for chapter in chapters_clean]\n",
    "\n",
    "print(\"Chapter 1 without contractions:\\n\", chapters_no_contractions[0][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28ca5558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter 1:\n",
      " Chapter I.   It is a truth universally acknowledged, that a single man in possession of a good fortune must be in want of a wife.  However little known the feelings or views of such a man may be on his first entering a neighbourhood, this truth is so well fixed in the minds of the surrounding families, that he is considered as the rightful property of some one or other of their daughters.  My dear Mr. Bennet, said his lady to him one day, have you heard that Netherfield Park is let at last?  Mr.\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the lines in each chapter\n",
    "chapters_continuous = [' '.join(chapter.splitlines()) for chapter in chapters_no_contractions]\n",
    "\n",
    "# Print a sample chapter\n",
    "print(\"Chapter 1:\\n\", chapters_continuous[0][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94b51d8",
   "metadata": {},
   "source": [
    "For each chapter, it splits the text into lines using `splitlines()`, then joins the lines back together using the `join()` function with a space as the separator. The result is a list of chapters, named `chapters_continuous`, where each chapter is a single continuous string without line breaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c70d0381",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mohsenrahimi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3e6346b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chapter I.',\n",
       " 'It is a truth universally acknowledged, that a single man in possession of a good fortune must be in want of a wife.',\n",
       " 'However little known the feelings or views of such a man may be on his first entering a neighbourhood, this truth is so well fixed in the minds of the surrounding families, that he is considered as the rightful property of some one or other of their daughters.',\n",
       " 'My dear Mr. Bennet, said his lady to him one day, have you heard that Netherfield Park is let at last?',\n",
       " 'Mr. Bennet replied that he had not.',\n",
       " 'But it is, returned she; for Mrs. Long has just been here, and she told me all about it.',\n",
       " 'Mr. Bennet made no answer.',\n",
       " 'Do not you want to know who has taken it?',\n",
       " 'cried his wife, impatiently.',\n",
       " 'You want to tell me, and I have no objection to hearing it.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "# Apply sentence tokenization to all chapters\n",
    "chapters_sentences = [sent_tokenize(chapter) for chapter in chapters_continuous]\n",
    "\n",
    "# Printthe first 10 tokenized sentences from Chapter I\n",
    "chapters_sentences[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5dbfdb",
   "metadata": {},
   "source": [
    "This code uses a list comprehension to apply the `sent_tokenize()` function to each chapter in the `chapters_continuous` list, creating a new list of chapters with tokenized sentences named `chapters_sentences`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9105ab0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a1fc739",
   "metadata": {},
   "source": [
    "To retain the dialogue structure in the text, you can use regular expressions to identify and extract the dialogues before applying the preprocessing steps. Here's a step-by-step guide on how to do this:\n",
    "\n",
    "1. Identify dialogue patterns in the text: Dialogues are usually enclosed in double quotes (\") or single quotes (') in books. You can create a regular expression pattern to match these dialogues.\n",
    "\n",
    "python\n",
    "import re\n",
    "\n",
    "#### Regular expression pattern for dialogues enclosed in double quotes\n",
    "dialogue_pattern = r'\"(.*?)\"'\n",
    "\n",
    "\n",
    "2. Extract dialogues from the chapters: You can use the `re.findall()` function to extract all the dialogues from each chapter.\n",
    "\n",
    "python\n",
    "##### Extract dialogues from all chapters\n",
    "chapters_dialogues = [re.findall(dialogue_pattern, chapter) for chapter in chapters_continuous]\n",
    "\n",
    "\n",
    "3. Replace dialogues in the chapters with placeholders: To keep the dialogue structure intact, replace the dialogues in each chapter with a placeholder (e.g., `DIALOGUE_PLACEHOLDER`). This will help you re-insert the dialogues after preprocessing.\n",
    "\n",
    "python\n",
    "##### Replace dialogues with placeholders\n",
    "chapters_no_dialogues = [re.sub(dialogue_pattern, \"DIALOGUE_PLACEHOLDER\", chapter) for chapter in chapters_continuous]\n",
    "\n",
    "\n",
    "4. Apply preprocessing steps to `chapters_no_dialogues`: Perform the preprocessing steps on the `chapters_no_dialogues` list, which now has placeholders instead of actual dialogues.\n",
    "\n",
    "5. Re-insert dialogues: After preprocessing, replace the `DIALOGUE_PLACEHOLDER` placeholders with the original dialogues stored in the `chapters_dialogues` list.\n",
    "\n",
    "python\n",
    "def replace_dialogue_placeholders(text, dialogues):\n",
    "    for dialogue in dialogues:\n",
    "        text = text.replace(\"DIALOGUE_PLACEHOLDER\", dialogue, 1)\n",
    "    return text\n",
    "\n",
    "chapters_preprocessed = [replace_dialogue_placeholders(preprocessed_chapter, dialogues) for preprocessed_chapter, dialogues in zip(chapters_preprocessed_without_dialogues, chapters_dialogues)]\n",
    "\n",
    "\n",
    "Now, you have a list of preprocessed chapters (`chapters_preprocessed`) with the dialogue structure retained. This will help you analyze character interactions and relationships more effectively.\n",
    "\n",
    "No I do not do this stage\n",
    "\n",
    "\n",
    "Alright, if you do not wish to retain the dialogue structure, you can continue with your preprocessing steps without considering the dialogues. The previous steps I provided for sentence tokenization and other preprocessing tasks do not specifically address dialogue structure, so you can proceed with those steps as they are.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a431c3e",
   "metadata": {},
   "source": [
    "### NER - Name Entity Recognition \n",
    "I have a list called `chapters_sentences`, where each element represents a list of tokenized sentences for each chapter, now I can process them using the `BERT-based NER model from Hugging Face's Transformers library`.\n",
    "I will iterate through each chapter and process each sentence with the NER pipeline to find the named entities in each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ff21d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load a pre-trained BERT-based NER model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "\n",
    "# Create a NER pipeline\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54313ac9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Chapter 1\n",
      "Ben I-PER\n",
      "##net I-PER\n",
      "Net I-LOC\n",
      "##her I-LOC\n",
      "##field I-LOC\n",
      "Park I-LOC\n",
      "Ben I-PER\n",
      "##net I-PER\n",
      "Long I-PER\n",
      "Ben I-PER\n",
      "##net I-PER\n",
      "George I-PER\n",
      "Allen I-PER\n",
      "Long I-PER\n",
      "Net I-LOC\n",
      "##her I-LOC\n",
      "##field I-LOC\n",
      "England I-LOC\n",
      "Morris I-PER\n",
      "Michael I-MISC\n",
      "Bing I-ORG\n",
      "##ley I-ORG\n",
      "Ben I-PER\n",
      "##net I-PER\n",
      "Bing I-PER\n",
      "##ley I-PER\n",
      "Bing I-PER\n",
      "##ley I-PER\n",
      "William I-PER\n",
      "Lucas I-PER\n",
      "Bing I-PER\n",
      "##ley I-PER\n",
      "Liz I-PER\n",
      "##zy I-PER\n",
      "Liz I-PER\n",
      "##zy I-PER\n",
      "Jane I-PER\n",
      "Lydia I-PER\n",
      "Liz I-PER\n",
      "##zy I-PER\n",
      "Ben I-PER\n",
      "##net I-PER\n",
      "Ben I-PER\n",
      "##net I-PER\n",
      "Ben I-PER\n",
      "##net I-PER\n",
      "George I-PER\n",
      "Allen I-PER\n",
      "Bing I-PER\n",
      "##ley I-PER\n",
      "George I-PER\n",
      "Allen I-PER\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Process each chapter's sentences\n",
    "for chapter_idx, sentences in enumerate(chapters_sentences[:1]):\n",
    "    print(f\"Processing Chapter {chapter_idx + 1}\")\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Extract named entities from the sentence\n",
    "        entities = ner_pipeline(sentence)\n",
    "        \n",
    "        # Print the named entities\n",
    "        for entity in entities:\n",
    "            print(entity[\"word\"], entity[\"entity\"])\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5ff3a7",
   "metadata": {},
   "source": [
    "It seems like the NER model has successfully identified some of the character names in Chapter II. \n",
    "However, some names are not correctly identified, and there are some extra characters (\"##\") in the output. \n",
    "These extra characters are a result of BERT's tokenization process, which splits words into subwords when needed.\n",
    "So I will post-process the NER output to merge these subwords and provide a cleaner output of the character names.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24781610",
   "metadata": {},
   "source": [
    "Now I need to filter out some entities such as, Mrs, Miss and King since I'm looking just for character's name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea9df5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_subwords(entities):\n",
    "    merged_entities = []\n",
    "    for entity in entities:\n",
    "        if entity[\"word\"].startswith(\"##\"):\n",
    "            if len(merged_entities) > 0:\n",
    "                merged_entities[-1][\"word\"] += entity[\"word\"][2:]\n",
    "        else:\n",
    "            merged_entities.append(entity)\n",
    "    return merged_entities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e34de61",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_titles = [\"Mrs\", \"Mr\", \"Miss\", \"Dr\", \"Sir\",\"King\"]\n",
    "\n",
    "def is_valid_entity(entity):\n",
    "    return entity[\"entity\"] == \"I-PER\" and entity[\"word\"] not in common_titles\n",
    "\n",
    "for chapter_idx, sentences in enumerate(chapters_sentences[:3]):\n",
    "    print(f\"Processing Chapter {chapter_idx + 1}\")\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Extract named entities from the sentence\n",
    "        entities = ner_pipeline(sentence)\n",
    "        \n",
    "        # Merge subwords and clean up the named entities\n",
    "        merged_entities = merge_subwords(entities)\n",
    "        \n",
    "        # Filter out unwanted entity types and common titles\n",
    "        filtered_entities = [entity for entity in merged_entities if is_valid_entity(entity)]\n",
    "        \n",
    "        # Print the named entities\n",
    "        for entity in filtered_entities:\n",
    "            print(entity[\"word\"], entity[\"entity\"])\n",
    "    print()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569f83a9",
   "metadata": {},
   "source": [
    "## co-occurrence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9cd3eada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import requests    # download the book \n",
      "import re          #`re` (regular expressions) library to remove unwanted characters and information from the text\n",
      "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
      "from transformers import pipeline\n",
      "import contractions\n",
      "from itertools import combinations\n",
      "from nltk.tokenize import sent_tokenize\n",
      "from nltk.corpus import stopwords\n",
      "import networkx as nx\n",
      "from time import time\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import nltk\n",
      "nltk.download('stopwords')\n",
      "import requests    # download the book \n",
      "book_id = 1342\n",
      "url = f\"https://www.gutenberg.org/files/{book_id}/{book_id}-0.txt\"\n",
      "response = requests.get(url)\n",
      "\n",
      "# Check if the request was successful\n",
      "if response.status_code == 200:\n",
      "    with open(\"book.txt\", \"w\", encoding=\"utf-8\") as f:\n",
      "        f.write(response.text)\n",
      "    print(\"Book downloaded successfully.\")\n",
      "else:\n",
      "    print(\"Failed to download the book.\")\n",
      "with open(\"book.txt\", \"r\", encoding=\"utf-8\") as f:\n",
      "    book_content = f.read()                        #`book_content` now contains the entire text of the book\n",
      "# Check the header and footer of the book\n",
      "print(\"The header is :\\n\",book_content[:500])\n",
      "\n",
      "print(\"\\nThe footer is :\\n\",book_content[-500:])\n",
      "#Remove page numbers, headers, and footers:\n",
      "\n",
      "book_content = re.sub(r\"^(.*?)\\*{3} START OF (.*?)\\*{3}\", \"\", book_content, flags=re.DOTALL)  # Remove header\n",
      "book_content = re.sub(r\"\\*{3} END OF (.*?)\\*{3}(\\s|.)*$\", \"\", book_content, flags=re.DOTALL)  # Remove footer\n",
      "# Check the content of the book after removing header and footer\n",
      "\n",
      "print(\"The header is :\\n\",book_content[:300])\n",
      "print(\"\\nThe footer is :\\n\",book_content[-300:])\n",
      "# Identify the start phrase\n",
      "with open(\"book.txt\", \"r\", encoding=\"utf-8\") as f:\n",
      "    book_content = f.read()                        #`book_content` now contains the entire text of the book\n",
      " \n",
      "# In below in each time I printed 10,000 characters to find out the starting point\n",
      "\n",
      "#print(book_content[1:30000])\n",
      "print(book_content[30000:40000])\n",
      "#print(book_content[40000:50000])\n",
      "print(\"The starting phrase is `Chapter I.]`\")\n",
      "# Identify the end phrase of the actual content\n",
      "#print(book_content[-5000:])\n",
      "#print(book_content[-10000:-5000])\n",
      "#print(book_content[-15000:-10000])\n",
      "print(book_content[-20000:-15000])\n",
      "print(\"The ending phrase is `THE END`\")\n",
      "with open(\"book.txt\", \"r\", encoding=\"utf-8\") as f:\n",
      "    book_content = f.read()                        #`book_content` now contains the entire text of the book\n",
      " \n",
      "\n",
      "start_phrase = \"Chapter I.]\"\n",
      "end_phrase = \"END OF THE PROJECT GUTENBERG EBOOK PRIDE AND PREJUDICE \"\n",
      "\n",
      "start_index = book_content.find(start_phrase)\n",
      "end_index = book_content.find(end_phrase)\n",
      "\n",
      "book_content = book_content[start_index:end_index]\n",
      "\n",
      "print(\"Check the content of the book after removing header and footer\\n\")\n",
      "print(\"The content is stated from:\\n\\n\",book_content[:500])\n",
      "print(\"\\nThe content is ended at:\\n\\n\",book_content[-500:])\n",
      "\n",
      "\n",
      "# Add an extra \"CHAPTER\" keyword at the end of the book for the last chapter\n",
      "#adding the \"CHAPTER\" keyword at the end. It seems like you have added it to ensure that the last chapter is \n",
      "#included correctly when splitting.\n",
      "\n",
      "book_content_with_last_chapter = book_content + \"CHAPTER\"\n",
      "\n",
      "# Split the book into chapters using the \"CHAPTER\" keyword\n",
      "chapters = book_content_with_last_chapter.split(\"CHAPTER\")\n",
      "\n",
      "# Remove the first element, which is just the text before the first chapter\n",
      "#chapters.pop(0)\n",
      "\n",
      "# Remove the last element, which is just the text after the last chapter\n",
      "chapters.pop()\n",
      "\n",
      "# Print the number of chapters and a sample chapter\n",
      "print(f\"Total chapters: {len(chapters)}\")\n",
      "# print the start and end of first and last chapter respectively\n",
      "\n",
      "print(\"\\nChapter 1 starts from:\\n\", chapters[0][:300])\n",
      "print(\"\\nChapter 59 ends at:\\n\\n\", chapters[59][-500:])\n",
      "import pandas as pd\n",
      "\n",
      "# Calculate the number of words in each chapter\n",
      "word_counts = [len(chapter.split()) for chapter in chapters]\n",
      "\n",
      "# Create a DataFrame with the chapter number and word count\n",
      "chapter_word_counts = pd.DataFrame({\"Chapter\": range(1, len(chapters) + 1), \"Word Count\": word_counts})\n",
      "#print(chapter_word_counts)\n",
      "import re\n",
      "#Remove special characters and numbers,    we use the `re.sub()` function. \n",
      "\n",
      "#This example removes all non-alphabetic characters except spaces and newline characters:\n",
      "\n",
      "#book_content_clean = re.sub(r\"[^a-zA-Z\\s\\n]\", \"\", book_content)\n",
      "import re\n",
      "# Function to clean the content of a chapter\n",
      "def clean_chapter_content(chapter_content):\n",
      "    return re.sub(r\"[^a-zA-Z\\s\\n.,;!?'\\\"]\", \"\", chapter_content)\n",
      "\n",
      "# Apply the cleaning function to all chapters\n",
      "chapters_clean = [clean_chapter_content(chapter) for chapter in chapters]\n",
      "\n",
      "# Print a sample cleaned chapter\n",
      "print(\"Cleaned Chapter 1 as below:\\n\", chapters_clean[0][:300])\n",
      "# apply the `contractions.fix()` function to each chapter (e.g., converting \"don't\" to \"do not\")\n",
      "\n",
      "import contractions\n",
      "\n",
      "chapters_no_contractions = [contractions.fix(chapter) for chapter in chapters_clean]\n",
      "\n",
      "print(\"Chapter 1 without contractions:\\n\", chapters_no_contractions[0][:500])\n",
      "# Concatenate the lines in each chapter\n",
      "chapters_continuous = [' '.join(chapter.splitlines()) for chapter in chapters_no_contractions]\n",
      "\n",
      "# Print a sample chapter\n",
      "print(\"Chapter 1:\\n\", chapters_continuous[0][:500])\n",
      "import nltk\n",
      "nltk.download('punkt')\n",
      "from nltk.tokenize import sent_tokenize\n",
      "# Apply sentence tokenization to all chapters\n",
      "chapters_sentences = [sent_tokenize(chapter) for chapter in chapters_continuous]\n",
      "\n",
      "# Printthe first 10 tokenized sentences from Chapter I\n",
      "chapters_sentences[0][:10]\n",
      "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
      "from transformers import pipeline\n",
      "\n",
      "# Load a pre-trained BERT-based NER model\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
      "model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
      "\n",
      "# Create a NER pipeline\n",
      "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
      "# Process each chapter's sentences\n",
      "for chapter_idx, sentences in enumerate(chapters_sentences[:1]):\n",
      "    print(f\"Processing Chapter {chapter_idx + 1}\")\n",
      "    \n",
      "    for sentence in sentences:\n",
      "        # Extract named entities from the sentence\n",
      "        entities = ner_pipeline(sentence)\n",
      "        \n",
      "        # Print the named entities\n",
      "        for entity in entities:\n",
      "            print(entity[\"word\"], entity[\"entity\"])\n",
      "    print()\n",
      "%history\n"
     ]
    }
   ],
   "source": [
    "%history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5e2c0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jupytext\n",
      "  Downloading jupytext-1.15.0-py3-none-any.whl (299 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.4/299.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nbformat in /Users/mohsenrahimi/anaconda3/lib/python3.11/site-packages (from jupytext) (5.7.0)\n",
      "Requirement already satisfied: pyyaml in /Users/mohsenrahimi/anaconda3/lib/python3.11/site-packages (from jupytext) (6.0)\n",
      "Requirement already satisfied: toml in /Users/mohsenrahimi/anaconda3/lib/python3.11/site-packages (from jupytext) (0.10.2)\n",
      "Requirement already satisfied: markdown-it-py>=1.0.0 in /Users/mohsenrahimi/anaconda3/lib/python3.11/site-packages (from jupytext) (2.2.0)\n",
      "Requirement already satisfied: mdit-py-plugins in /Users/mohsenrahimi/anaconda3/lib/python3.11/site-packages (from jupytext) (0.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/mohsenrahimi/anaconda3/lib/python3.11/site-packages (from markdown-it-py>=1.0.0->jupytext) (0.1.0)\n",
      "Requirement already satisfied: fastjsonschema in /Users/mohsenrahimi/anaconda3/lib/python3.11/site-packages (from nbformat->jupytext) (2.16.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /Users/mohsenrahimi/anaconda3/lib/python3.11/site-packages (from nbformat->jupytext) (4.17.3)\n",
      "Requirement already satisfied: jupyter-core in /Users/mohsenrahimi/anaconda3/lib/python3.11/site-packages (from nbformat->jupytext) (5.3.0)\n",
      "Requirement already satisfied: traitlets>=5.1 in /Users/mohsenrahimi/anaconda3/lib/python3.11/site-packages (from nbformat->jupytext) (5.7.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/mohsenrahimi/anaconda3/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat->jupytext) (22.1.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /Users/mohsenrahimi/anaconda3/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat->jupytext) (0.18.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /Users/mohsenrahimi/anaconda3/lib/python3.11/site-packages (from jupyter-core->nbformat->jupytext) (2.5.2)\n",
      "Installing collected packages: jupytext\n",
      "Successfully installed jupytext-1.15.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    " pip install jupytext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cfea9460",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1291984116.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[25], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    jupyter nbextension install --py jupytext --user\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "jupyter nbextension install --py jupytext --user\n",
    "   jupyter nbextension enable --py jupytext --user\n",
    "   jupyter serverextension enable --py jupytext --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a0fcbae4",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (304255055.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[26], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    jupyter nbextension install --py jupytext --user\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "jupyter nbextension install --py jupytext --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb3b0d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
